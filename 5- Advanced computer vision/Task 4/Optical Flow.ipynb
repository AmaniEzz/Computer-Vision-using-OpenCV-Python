{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optical Flow with Lucas-Kanade method Using | Python OpenCV\n",
    "\n",
    "\n",
    "## What is the Optical Flow in motion tracking?\n",
    "\n",
    "Optical flow is the pattern of apparent motion of image objects between two consecutive frames caused by the movemement of object or camera. It is 2D vector field where each vector is a displacement vector showing the movement of points from first frame to second.\n",
    "\n",
    "#### Optical flow has many applications in areas like :\n",
    "\n",
    "- Structure from Motion\n",
    "- Video Compression\n",
    "- Video Stabilization ...\n",
    "\n",
    "\n",
    "\n",
    "OpenCV provides a single function, `cv2.calcOpticalFlowPyrLK()`.\n",
    "\n",
    "\n",
    "**Syntax:** `cv.calcOpticalFlowPyrLK(prevImg, nextImg, prevPts, nextPts[, status[, err[, winSize[, maxLevel[, criteria[, flags[, minEigThreshold]]]]]]])`\n",
    "\n",
    "**Parameters:**\n",
    "\n",
    "- ***prevImg :–*** first 8-bit input image or pyramid constructed by buildOpticalFlowPyramid.\n",
    "- ***nextImg :–*** second input image or pyramid of the same size and the same type as prevImg.\n",
    "- ***prevPts :–*** vector of 2D points for which the flow needs to be found; point coordinates must be single-precision floating-point numbers.\n",
    "\n",
    "- ***winSize :–*** size of the search window at each pyramid level.\n",
    "- ***maxLevel :–***  0-based maximal pyramid level number; if set to 0, pyramids are not used (single level), if set to 1, two levels are used, and so on; if pyramids are passed to input then algorithm will use as many levels as pyramids have but no more than maxLevel.\n",
    "\n",
    "- ***criteria :–*** parameter, specifying the termination criteria of the iterative search algorithm (after the specified maximum number of iterations criteria.maxCount or when the search window moves by less than criteria.epsilon.\n",
    "\n",
    "- ***flags:-***\n",
    "       \n",
    "    - **OPTFLOW_USE_INITIAL_FLOW** uses initial estimations, stored in nextPts; if the flag is not set, then prevPts is copied to nextPts and is considered the initial estimate.\n",
    "    - **OPTFLOW_LK_GET_MIN_EIGENVALS** use minimum eigen values as an error measure (see minEigThreshold description); if the flag is not set, then L1 distance between patches around the original and a moved point, divided by number of pixels in a window, is used as a error measure.\n",
    "\n",
    "\n",
    "- ***minEigThreshold :-***\tthe algorithm calculates the minimum eigen value of a 2x2 normal matrix of optical flow equations (this matrix is called a spatial gradient matrix), divided by number of pixels in a window; if this value is less than minEigThreshold, then a corresponding feature is filtered out and its flow is not processed, so it allows to remove bad points and get a performance boost.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Return:**\n",
    "\n",
    "- ***nextPts :-*** output vector of 2D points (with single-precision floating-point coordinates) containing the calculated new positions of input features in the second image; when **OPTFLOW_USE_INITIAL_FLOW** flag is passed, the vector must have the same size as in the input.\n",
    "\n",
    "\n",
    "- ***status :–*** output status vector (of unsigned chars); each element of the vector is set to 1 if the flow for the corresponding features has been found, otherwise, it is set to 0.\n",
    "\n",
    "\n",
    "- ***err :–*** output vector of errors; each element of the vector is set to an error for the corresponding feature, type of the error measure can be set in flags parameter; if the flow wasn’t found then the error is not defined (use the status parameter to find such cases).\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "Here, we create a simple application which tracks some points in a video. To decide the points, we use cv2.goodFeaturesToTrack(). We take the first frame, detect some Shi-Tomasi corner points in it, then we iteratively track those points using Lucas-Kanade optical flow. For the function cv2.calcOpticalFlowPyrLK() we pass the previous frame, previous points and next frame. It returns next points along with some status numbers which has a value of 1 if next point is found, else zero. We iteratively pass these next points as previous points in next step. See the code below:\n",
    "\n",
    "\n",
    "\n",
    "## Search for sift and pyramids..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.4.0) C:\\Users\\appveyor\\AppData\\Local\\Temp\\1\\pip-req-build-52oirelq\\opencv\\modules\\imgproc\\src\\color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cv::cvtColor'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-ff54783046f4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;31m# Take first frame and find corners in it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[0mret\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mold_frame\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcap\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m \u001b[0mold_gray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mold_frame\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCOLOR_BGR2GRAY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[0mp0\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgoodFeaturesToTrack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mold_gray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfeature_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.4.0) C:\\Users\\appveyor\\AppData\\Local\\Temp\\1\\pip-req-build-52oirelq\\opencv\\modules\\imgproc\\src\\color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cv::cvtColor'\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "cap = cv2.VideoCapture('slow.flv')\n",
    "\n",
    "# params for ShiTomasi corner detection\n",
    "feature_params = dict( maxCorners = 100,\n",
    "                       qualityLevel = 0.3,\n",
    "                       minDistance = 7,\n",
    "                       blockSize = 7 )\n",
    "\n",
    "# Parameters for lucas kanade optical flow\n",
    "lk_params = dict( winSize  = (15,15),\n",
    "                  maxLevel = 2,\n",
    "                  criteria = (cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 0.03))\n",
    "\n",
    "# Create some random colors\n",
    "color = np.random.randint(0,255,(100,3))\n",
    "\n",
    "# Take first frame and find corners in it\n",
    "ret, old_frame = cap.read()\n",
    "old_gray = cv2.cvtColor(old_frame, cv2.COLOR_BGR2GRAY)\n",
    "p0 = cv2.goodFeaturesToTrack(old_gray, mask = None, **feature_params)\n",
    "\n",
    "# Create a mask image for drawing purposes\n",
    "mask = np.zeros_like(old_frame)\n",
    "\n",
    "while(1):\n",
    "    ret,frame = cap.read()\n",
    "    frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # calculate optical flow\n",
    "    p1, st, err = cv2.calcOpticalFlowPyrLK(old_gray, frame_gray, p0, None, **lk_params)\n",
    "\n",
    "    # Select good points\n",
    "    good_new = p1[st==1]\n",
    "    good_old = p0[st==1]\n",
    "\n",
    "    # draw the tracks\n",
    "    for i,(new,old) in enumerate(zip(good_new,good_old)):\n",
    "        a,b = new.ravel()\n",
    "        c,d = old.ravel()\n",
    "        mask = cv2.line(mask, (a,b),(c,d), color[i].tolist(), 2)\n",
    "        frame = cv2.circle(frame,(a,b),5,color[i].tolist(),-1)\n",
    "    img = cv2.add(frame,mask)\n",
    "\n",
    "    cv2.imshow('frame',img)\n",
    "    k = cv2.waitKey(30) & 0xff\n",
    "    if k == 27:\n",
    "        break\n",
    "\n",
    "    # Now update the previous frame and previous points\n",
    "    old_gray = frame_gray.copy()\n",
    "    p0 = good_new.reshape(-1,1,2)\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dense Optical flow\n",
    "\n",
    "Lucas-Kanade method computes optical flow for a sparse feature set (in our example, corners detected using Shi-Tomasi algorithm). OpenCV provides another algorithm to find the dense optical flow. It computes the optical flow vector for every pixel of the frame which may be responsible for its slow speed but leading to a better accurate result. There can be various kinds of implementations of dense optical flow. The example below will follow the Farneback method along with OpenCV.\n",
    "\n",
    "\n",
    "### Franeback Method\n",
    "\n",
    "- The first step is that the method approximates the windows of image frames by a quadratic polynomial with the help of the polynomial expansion transform. \n",
    "\n",
    "- Next, by observing how the polynomial transforms under the state of motion. i.e. to estimate displacement fields. Dense optical flow is computed, after a series of refinements.\n",
    "\n",
    "### For OpenCV’s implementation\n",
    "\n",
    "Below sample shows how to find the dense optical flow using above algorithm. We get a 2-channel array with optical flow vectors, (u,v). We find their magnitude and direction. We color code the result for better visualization. Direction corresponds to Hue value of the image. Magnitude corresponds to Value plane.  The strength of HSV is always set to a maximum of 255 for optimal visibility. See the code below:\n",
    "\n",
    "#### Syntax: \n",
    "`cv.calcOpticalFlowFarneback(prev, next, flow, pyr_scale, levels, winsize, iterations, poly_n, poly_sigma, flags)`.\n",
    "\n",
    "#### Parameters\n",
    "\n",
    "- **prev :-**\tfirst 8-bit single-channel input image.\n",
    "- **next :-** second input image of the same size and the same type as prev.\n",
    "- **flow :-**\tcomputed flow image that has the same size as prev and type CV_32FC2.\n",
    "- **pyr_scale :-**\tparameter, specifying the image scale (<1) to build pyramids for each image; pyr_scale=0.5 means a classical pyramid, where each next layer is twice smaller than the previous one.\n",
    "\n",
    "- **levels :-**\tnumber of pyramid layers including the initial image; levels=1 means that no extra layers are created and only the original images are used.\n",
    "\n",
    "- **winsize :-**\taveraging window size; larger values increase the algorithm robustness to image noise and give more chances for fast motion detection, but yield more blurred motion field.\n",
    "\n",
    "- **iterations :-**\tnumber of iterations the algorithm does at each pyramid level.\n",
    "\n",
    "- **poly_n :-**\tsize of the pixel neighborhood used to find polynomial expansion in each pixel; larger values mean that the image will be approximated with smoother surfaces, yielding more robust algorithm and more blurred motion field, typically poly_n =5 or 7.\n",
    "\n",
    "- **poly_sigma :-**\tstandard deviation of the Gaussian that is used to smooth derivatives used as a basis for the polynomial expansion; for poly_n=5, you can set poly_sigma=1.1, for poly_n=7, a good value would be poly_sigma=1.5.\n",
    "\n",
    "- **flags :-** operation flags that can be a combination of the following:\n",
    "    - ***OPTFLOW_USE_INITIAL_FLOW*** uses the input flow as an initial flow approximation.\n",
    "    - ***OPTFLOW_FARNEBACK_GAUSSIAN*** uses the Gaussian winsize×winsize filter instead of a box filter of the same size for optical flow estimation; usually, this option gives z more accurate flow than with a box filter, at the cost of lower speed; normally, winsize for a Gaussian window should be set to a larger value to achieve the same level of robustness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[https://docs.opencv.org/master/dc/d6b/group__video__track.html#ga473e4b886d0bcc6b65831eb88ed93323](https://docs.opencv.org/master/dc/d6b/group__video__track.html#ga473e4b886d0bcc6b65831eb88ed93323)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv \n",
    "import numpy as np \n",
    "  \n",
    "\n",
    "# The video feed is read in as a VideoCapture object \n",
    "cap = cv.VideoCapture(\"videoplayback.mp4\") \n",
    "  \n",
    "# ret = a boolean return value from getting the frame, \n",
    "# first_frame = the first frame in the entire video sequence \n",
    "ret, first_frame = cap.read() \n",
    "  \n",
    "# Converts frame to grayscale- less computationally expensive \n",
    "prev_gray = cv.cvtColor(first_frame, cv.COLOR_BGR2GRAY) \n",
    "  \n",
    "# Creates an image filled with zero intensities with the same dimensions as the frame \n",
    "mask = np.zeros_like(first_frame) \n",
    "  \n",
    "# Sets image saturation to maximum \n",
    "mask[..., 1] = 255\n",
    "  \n",
    "while(cap.isOpened()): \n",
    "      \n",
    "    # frame = the current frame being projected in the video \n",
    "    ret, frame = cap.read() \n",
    "      \n",
    "    # Opens a new window and displays the input frame \n",
    "    cv.imshow(\"input\", frame) \n",
    "      \n",
    "    # Converts each frame to grayscale - we previously  \n",
    "    # only converted the first frame to grayscale \n",
    "    gray = cv.cvtColor(frame, cv.COLOR_BGR2GRAY) \n",
    "      \n",
    "    # Calculates dense optical flow by Farneback method \n",
    "    flow = cv.calcOpticalFlowFarneback(prev_gray, gray,  None, 0.5,  3,  15,  3,  5,  1.2,  0) \n",
    "      \n",
    "    # Computes the magnitude and angle of the 2D vectors \n",
    "    magnitude, angle = cv.cartToPolar(flow[..., 0], flow[..., 1]) \n",
    "      \n",
    "    # Sets image hue according to the optical flow  direction \n",
    "    mask[..., 0] = angle * 180 / np.pi / 2\n",
    "      \n",
    "    # Sets image value according to the optical flow magnitude (normalized) \n",
    "    mask[..., 2] = cv.normalize(magnitude, None, 0, 255, cv.NORM_MINMAX) \n",
    "      \n",
    "    # Converts HSV to RGB (BGR) color representation \n",
    "    rgb = cv.cvtColor(mask, cv.COLOR_HSV2BGR) \n",
    "      \n",
    "    # Opens a new window and displays the output frame \n",
    "    cv.imshow(\"dense optical flow\", rgb) \n",
    "      \n",
    "    # Updates previous frame \n",
    "    prev_gray = gray \n",
    "    \n",
    "    if cv.waitKey(1) & 0xFF == ord('q'): \n",
    "        break\n",
    "\n",
    "# The following frees up resources and closes all windows \n",
    "cap.release() \n",
    "cv.destroyAllWindows() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
